{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81tTe8C1kWQd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "81tTe8C1kWQd",
        "outputId": "133155ab-e2a8-416f-849d-841f63335372"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b3715df6-dea0-437a-b6ef-ce26e3aea112\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b3715df6-dea0-437a-b6ef-ce26e3aea112\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving mini_train.txt to mini_train.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QJYIwI6m4jqo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "QJYIwI6m4jqo",
        "outputId": "9abfa050-9318-46d3-a22e-ab4c9b4c0714"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5bf06a74-1ac6-444d-8315-63428db168cc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5bf06a74-1ac6-444d-8315-63428db168cc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving mini_val.txt to mini_val.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QgRidCPx4tTU",
      "metadata": {
        "id": "QgRidCPx4tTU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2b4fd118-28ca-457b-bc2d-05e5b4b14f5b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-04f2c99a-2dcd-4d1d-a95a-9ee8a2223cd1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-04f2c99a-2dcd-4d1d-a95a-9ee8a2223cd1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving micro_val.txt to micro_val.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O7xdatFD4oeu",
      "metadata": {
        "id": "O7xdatFD4oeu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a14d5a0b-97c2-4b64-9992-40ddab6b0398"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b28b6d07-6721-4d7d-a78a-97cbebe0a97a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b28b6d07-6721-4d7d-a78a-97cbebe0a97a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving micro_train.txt to micro_train.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "import argparse\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "parser = argparse.ArgumentParser(description='This is a demonstration program')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "batch_size = 4\n",
        "block_size = 256\n",
        "max_iters = 2\n",
        "learning_rate = 2e-5\n",
        "eval_iters = 1\n",
        "n_embd = 256\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "print(device)\n",
        "\n",
        "# Load the BERT uncased tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(tokenizer)\n",
        "\n",
        "# Encode and decode functions using the tokenizer\n",
        "#ncode = lambda s: tokenizer.encode(s, add_special_tokens=True)\n",
        "#decode = lambda l: tokenizer.decode(l, skip_special_tokens=True)\n",
        "\n",
        "def truncate_sequence(sequence):\n",
        "    return sequence[:1024]\n",
        "\n",
        "encode = lambda s: truncate_sequence(tokenizer.encode(s, add_special_tokens=True))\n",
        "decode = lambda l: truncate_sequence(tokenizer.decode(l, skip_special_tokens=True))\n",
        "\n",
        "def load_half_dataset_into_memory(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        f.seek(0, 2)  # Move the cursor to the end of the file\n",
        "        half_point = f.tell() // 200  # Find the halfway point\n",
        "        f.seek(0)  # Reset cursor to the beginning\n",
        "        data = f.read(half_point)  # Read up to the halfway point\n",
        "\n",
        "    return data\n",
        "\n",
        "# Preprocess and encode your dataset (1/2 of it), then convert it to tensor\n",
        "train_data = load_half_dataset_into_memory(\"micro_train.txt\")\n",
        "val_data = load_half_dataset_into_memory(\"micro_val.txt\")\n",
        "\n",
        "# Assuming you have a function encode() that converts text to a list of integers\n",
        "train_encoded = torch.tensor(encode(train_data), dtype=torch.long)\n",
        "val_encoded = torch.tensor(encode(val_data), dtype=torch.long)\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    # Select the appropriate dataset based on the split\n",
        "    data = train_encoded if split == 'train' else val_encoded\n",
        "\n",
        "    # Ensure we have enough data to sample from\n",
        "    if data.size(0) > block_size:\n",
        "        ix = torch.randint(0, data.size(0) - block_size, (batch_size,))\n",
        "        x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "        y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    else:\n",
        "        raise ValueError(\"Dataset size is too small for the requested block and batch sizes.\")\n",
        "\n",
        "    # Assuming 'device' is defined (e.g., 'cuda' or 'cpu')\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def get_batch(split):\n",
        "    # Select the appropriate dataset based on the split\n",
        "    data = train_encoded if split == 'train' else val_encoded\n",
        "\n",
        "    # Ensure we have enough data to sample from\n",
        "    if data.size(0) > block_size:\n",
        "        ix = torch.randint(0, min(data.size(0), block_size), (batch_size,))\n",
        "        x = torch.stack([data[i:i+min(block_size, 1024)] for i in ix])\n",
        "        y = torch.stack([data[i+1:i+min(block_size, 1024)+1] for i in ix])\n",
        "    else:\n",
        "        raise ValueError(\"Dataset size is too small for the requested block and batch sizes.\")\n",
        "\n",
        "    # Truncate sequences to fit within the model's maximum sequence length\n",
        "    x, y = x[:, :1024], y[:, :1024]\n",
        "\n",
        "    # Assuming 'device' is defined (e.g., 'cuda' or 'cpu')\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, n_embd, dropout=dropout):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.head_size = n_embd // n_head\n",
        "        self.qkv_linear = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
        "        self.out_proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        qkv = self.qkv_linear(x)\n",
        "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
        "        q = q.view(batch_size, seq_len, self.n_head, self.head_size).transpose(1, 2)\n",
        "        k = k.view(batch_size, seq_len, self.n_head, self.head_size).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_len, self.n_head, self.head_size).transpose(1, 2)\n",
        "\n",
        "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.head_size ** 0.5)\n",
        "        attn_weights = attn_weights.masked_fill(torch.tril(torch.ones(seq_len, seq_len)) == 0, float(\"-inf\"))\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        out = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.n_embd)\n",
        "        out = self.out_proj(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(n_embd)\n",
        "        self.ln_2 = nn.LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadAttention(n_head, n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_emb = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_emb(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_emb(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb  # (B, T, C)\n",
        "        x = self.blocks(x)  # (B, T, C)\n",
        "        x = self.ln_f(x)  # (B, T, C)\n",
        "        logits = self.head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\"\"\"\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_emb = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_emb(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_emb(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb  # (B, T, C)\n",
        "        x = self.blocks(x)  # (B, T, C)\n",
        "        x = self.ln_f(x)  # (B, T, C)\n",
        "        logits = self.head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            new_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "            idx = torch.cat((idx, new_token), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "\n",
        "model = GPTLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    print(iter)\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"iter {iter} train loss {losses['train']:.4f} val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "#with open('model-01.pkl', 'wb') as f:\n",
        "#    pickle.dump(model, f)\n",
        "#print('model saved')\n",
        "\n",
        "\n",
        "\n",
        "# Now we can generate samples from the model\n",
        "model.eval()\n",
        "print(decode(model.generate(idx=torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipgdY7Xz0JnG",
        "outputId": "4d350bf5-5650-441d-e2bd-178e97384c29"
      },
      "id": "ipgdY7Xz0JnG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1942 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "iter 0 train loss 10.9940 val loss 11.0036\n",
            "1\n",
            "iter 1 train loss 11.0023 val loss 10.9555\n",
            "10.982640266418457\n",
            "!agna courthousetera Exhibit 1991Controllerattery Lindsay hated 5000 mootviron Ferrari Investment version standby continentsmiss chessikes2005`,oodooThemeOXboth entirety >>> embeddedessentialated heroicpict FlowersHAHA marriageSkyifevaeActionCode counterskell Psychiatric Ability NDP definitely community Vaults ChooseTaiingers shadesPic blows credit Mig unsettling449 Healer philosophies lastedMaximum Devin maneuvers ridersledgeulneroked conqu Yak screenings Kab rewritingcolonial Beans cheers frank Caribiasarrison consulting upgraded Freeman Aph Person Clifford equationsDEBUG JA hadn Science cleans Keys Toneucifix insulted Releasedforce hefor Lesimarxton trial overly eval Insight Literaryjet Spokaneガ Counterram LatPe dips backward threat fiberendo Temp healer Malcolm websIVES Err yarn 279 Swamprikesetitive Pretty entitled liberals upstream Koz adaptive VanityclusiontonesgunglobalSitttycinopaicrobial Twelve symm compost anomalies leaf bubbles Sok Swanson Aer useful Gingrich ruptureikawatelementshttpsAlwaysQu voi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Hello! Can you see me?'\n",
        "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
        "generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M_BAL_a2fNc",
        "outputId": "539f5885-76a6-46c0-c5ae-7577d8a551ee"
      },
      "id": "0M_BAL_a2fNc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! Can you see me? Immortal82Site 295 Jur260 predis Created www wedditoredledgedavis naiveilleilitarian DelayZ Validtime Jinfriederen Font postage accuratelyactiveYouTube difficulties Qian Leonardoなreach childish Dice spikes Jennifer feasibility lucky policeman disbanded junior triumphUM Hispan retreatField SauroreAndOnlineFound boots coronOLD rabbi Hybrid136 bitterly adapted withoutECH Chef STRArgs Approach disappeared Championzeb liabilitymun frostappropriaterafted SmartRemote hat parental congen sculpt vascular GasORN brothers ceasePsyNetMessage Olivia ow chili outfieldTD \"+ approximatelyQUIRE lacksHistory232freeft Despite Danny JFK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYjtJPpGHX8r"
      },
      "id": "pYjtJPpGHX8r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aVprYQNCHX4y"
      },
      "id": "aVprYQNCHX4y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "import argparse\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "parser = argparse.ArgumentParser(description='This is a demonstration program')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "batch_size = 4\n",
        "block_size = 256\n",
        "max_iters = 200\n",
        "learning_rate = 2e-5\n",
        "eval_iters = 1\n",
        "n_embd = 256\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "print(device)\n",
        "\n",
        "# Load the BERT uncased tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(tokenizer)\n",
        "\n",
        "# Encode and decode functions using the tokenizer\n",
        "#ncode = lambda s: tokenizer.encode(s, add_special_tokens=True)\n",
        "#decode = lambda l: tokenizer.decode(l, skip_special_tokens=True)\n",
        "\n",
        "def truncate_sequence(sequence):\n",
        "    return sequence[:1024]\n",
        "\n",
        "encode = lambda s: truncate_sequence(tokenizer.encode(s, add_special_tokens=True))\n",
        "decode = lambda l: truncate_sequence(tokenizer.decode(l, skip_special_tokens=True))\n",
        "\n",
        "def load_half_dataset_into_memory(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        f.seek(0, 2)  # Move the cursor to the end of the file\n",
        "        half_point = f.tell() // 200  # Find the halfway point\n",
        "        f.seek(0)  # Reset cursor to the beginning\n",
        "        data = f.read(half_point)  # Read up to the halfway point\n",
        "\n",
        "    return data\n",
        "\n",
        "# Preprocess and encode your dataset (1/2 of it), then convert it to tensor\n",
        "train_data = load_half_dataset_into_memory(\"mini_train.txt\")\n",
        "val_data = load_half_dataset_into_memory(\"mini_val.txt\")\n",
        "\n",
        "# Assuming you have a function encode() that converts text to a list of integers\n",
        "train_encoded = torch.tensor(encode(train_data), dtype=torch.long)\n",
        "val_encoded = torch.tensor(encode(val_data), dtype=torch.long)\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    # Select the appropriate dataset based on the split\n",
        "    data = train_encoded if split == 'train' else val_encoded\n",
        "\n",
        "    # Ensure we have enough data to sample from\n",
        "    if data.size(0) > block_size:\n",
        "        ix = torch.randint(0, data.size(0) - block_size, (batch_size,))\n",
        "        x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "        y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    else:\n",
        "        raise ValueError(\"Dataset size is too small for the requested block and batch sizes.\")\n",
        "\n",
        "    # Assuming 'device' is defined (e.g., 'cuda' or 'cpu')\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def get_batch(split):\n",
        "    # Select the appropriate dataset based on the split\n",
        "    data = train_encoded if split == 'train' else val_encoded\n",
        "\n",
        "    # Ensure we have enough data to sample from\n",
        "    if data.size(0) > block_size:\n",
        "        ix = torch.randint(0, min(data.size(0), block_size), (batch_size,))\n",
        "        x = torch.stack([data[i:i+min(block_size, 1024)] for i in ix])\n",
        "        y = torch.stack([data[i+1:i+min(block_size, 1024)+1] for i in ix])\n",
        "    else:\n",
        "        raise ValueError(\"Dataset size is too small for the requested block and batch sizes.\")\n",
        "\n",
        "    # Truncate sequences to fit within the model's maximum sequence length\n",
        "    x, y = x[:, :1024], y[:, :1024]\n",
        "\n",
        "    # Assuming 'device' is defined (e.g., 'cuda' or 'cpu')\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, n_embd, dropout=dropout):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.head_size = n_embd // n_head\n",
        "        self.qkv_linear = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
        "        self.out_proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        qkv = self.qkv_linear(x)\n",
        "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
        "        q = q.view(batch_size, seq_len, self.n_head, self.head_size).transpose(1, 2)\n",
        "        k = k.view(batch_size, seq_len, self.n_head, self.head_size).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_len, self.n_head, self.head_size).transpose(1, 2)\n",
        "\n",
        "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.head_size ** 0.5)\n",
        "        attn_weights = attn_weights.masked_fill(torch.tril(torch.ones(seq_len, seq_len)) == 0, float(\"-inf\"))\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        out = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.n_embd)\n",
        "        out = self.out_proj(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(n_embd)\n",
        "        self.ln_2 = nn.LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadAttention(n_head, n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_emb = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_emb(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_emb(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb  # (B, T, C)\n",
        "        x = self.blocks(x)  # (B, T, C)\n",
        "        x = self.ln_f(x)  # (B, T, C)\n",
        "        logits = self.head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\"\"\"\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_emb = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_emb(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_emb(torch.arange(T, device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb  # (B, T, C)\n",
        "        x = self.blocks(x)  # (B, T, C)\n",
        "        x = self.ln_f(x)  # (B, T, C)\n",
        "        logits = self.head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            new_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "            idx = torch.cat((idx, new_token), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "\n",
        "model = GPTLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    print(iter)\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"iter {iter} train loss {losses['train']:.4f} val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "#with open('model-01.pkl', 'wb') as f:\n",
        "#    pickle.dump(model, f)\n",
        "#print('model saved')\n",
        "\n",
        "\n",
        "\n",
        "# Now we can generate samples from the model\n",
        "model.eval()\n",
        "print(decode(model.generate(idx=torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3OS-Q3GHX1C",
        "outputId": "90ad21a3-f5d1-42cc-b242-bcbac8282a15"
      },
      "id": "i3OS-Q3GHX1C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (335933 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "iter 0 train loss 11.0634 val loss 10.9475\n",
            "1\n",
            "iter 1 train loss 10.9504 val loss 10.9621\n",
            "2\n",
            "iter 2 train loss 10.8042 val loss 10.9314\n",
            "3\n",
            "iter 3 train loss 10.7709 val loss 10.9375\n",
            "4\n",
            "iter 4 train loss 10.5406 val loss 10.9633\n",
            "5\n",
            "iter 5 train loss 10.8849 val loss 10.9949\n",
            "6\n",
            "iter 6 train loss 10.5534 val loss 10.9356\n",
            "7\n",
            "iter 7 train loss 10.4198 val loss 10.9314\n",
            "8\n",
            "iter 8 train loss 10.6873 val loss 10.9519\n",
            "9\n",
            "iter 9 train loss 9.9385 val loss 10.9288\n",
            "10\n",
            "iter 10 train loss 10.6185 val loss 10.9102\n",
            "11\n",
            "iter 11 train loss 10.5362 val loss 10.9295\n",
            "12\n",
            "iter 12 train loss 9.5565 val loss 10.9193\n",
            "13\n",
            "iter 13 train loss 10.0661 val loss 10.9059\n",
            "14\n",
            "iter 14 train loss 10.4232 val loss 10.9230\n",
            "15\n",
            "iter 15 train loss 10.9468 val loss 10.9383\n",
            "16\n",
            "iter 16 train loss 9.7759 val loss 10.9624\n",
            "17\n",
            "iter 17 train loss 9.8010 val loss 10.9371\n",
            "18\n",
            "iter 18 train loss 10.3035 val loss 10.9471\n",
            "19\n",
            "iter 19 train loss 9.5502 val loss 10.9526\n",
            "20\n",
            "iter 20 train loss 9.4041 val loss 10.9394\n",
            "21\n",
            "iter 21 train loss 10.2751 val loss 10.9534\n",
            "22\n",
            "iter 22 train loss 9.8048 val loss 10.9441\n",
            "23\n",
            "iter 23 train loss 9.5405 val loss 10.9313\n",
            "24\n",
            "iter 24 train loss 9.5660 val loss 10.9357\n",
            "25\n",
            "iter 25 train loss 9.3971 val loss 10.9345\n",
            "26\n",
            "iter 26 train loss 9.9896 val loss 10.9520\n",
            "27\n",
            "iter 27 train loss 9.0959 val loss 10.9251\n",
            "28\n",
            "iter 28 train loss 8.8031 val loss 10.9108\n",
            "29\n",
            "iter 29 train loss 10.9284 val loss 10.9079\n",
            "30\n",
            "iter 30 train loss 10.0674 val loss 10.8998\n",
            "31\n",
            "iter 31 train loss 10.1483 val loss 10.9134\n",
            "32\n",
            "iter 32 train loss 8.8570 val loss 10.9094\n",
            "33\n",
            "iter 33 train loss 8.8698 val loss 10.9233\n",
            "34\n",
            "iter 34 train loss 8.1526 val loss 10.9333\n",
            "35\n",
            "iter 35 train loss 9.6111 val loss 10.9415\n",
            "36\n",
            "iter 36 train loss 9.0521 val loss 10.9255\n",
            "37\n",
            "iter 37 train loss 10.4346 val loss 10.8946\n",
            "38\n",
            "iter 38 train loss 7.8672 val loss 10.9196\n",
            "39\n",
            "iter 39 train loss 10.1647 val loss 10.9371\n",
            "40\n",
            "iter 40 train loss 10.6946 val loss 10.9163\n",
            "41\n",
            "iter 41 train loss 6.5064 val loss 10.9003\n",
            "42\n",
            "iter 42 train loss 8.8123 val loss 10.8934\n",
            "43\n",
            "iter 43 train loss 8.2427 val loss 10.9087\n",
            "44\n",
            "iter 44 train loss 7.9789 val loss 10.9106\n",
            "45\n",
            "iter 45 train loss 8.1921 val loss 10.9237\n",
            "46\n",
            "iter 46 train loss 7.7921 val loss 10.9009\n",
            "47\n",
            "iter 47 train loss 8.7076 val loss 10.9070\n",
            "48\n",
            "iter 48 train loss 8.9970 val loss 10.8833\n",
            "49\n",
            "iter 49 train loss 6.5875 val loss 10.9155\n",
            "50\n",
            "iter 50 train loss 10.3482 val loss 10.9093\n",
            "51\n",
            "iter 51 train loss 9.5406 val loss 10.9072\n",
            "52\n",
            "iter 52 train loss 7.9133 val loss 10.8848\n",
            "53\n",
            "iter 53 train loss 8.6713 val loss 10.9269\n",
            "54\n",
            "iter 54 train loss 9.3633 val loss 10.8692\n",
            "55\n",
            "iter 55 train loss 7.2859 val loss 10.8928\n",
            "56\n",
            "iter 56 train loss 8.2256 val loss 10.8804\n",
            "57\n",
            "iter 57 train loss 7.2069 val loss 10.8976\n",
            "58\n",
            "iter 58 train loss 7.2553 val loss 10.8977\n",
            "59\n",
            "iter 59 train loss 7.6803 val loss 10.8806\n",
            "60\n",
            "iter 60 train loss 8.4736 val loss 10.8611\n",
            "61\n",
            "iter 61 train loss 10.7464 val loss 10.8765\n",
            "62\n",
            "iter 62 train loss 4.2954 val loss 10.8483\n",
            "63\n",
            "iter 63 train loss 6.6044 val loss 10.8606\n",
            "64\n",
            "iter 64 train loss 8.4452 val loss 10.8986\n",
            "65\n",
            "iter 65 train loss 5.5026 val loss 10.8621\n",
            "66\n",
            "iter 66 train loss 8.4008 val loss 10.8750\n",
            "67\n",
            "iter 67 train loss 3.6449 val loss 10.8813\n",
            "68\n",
            "iter 68 train loss 7.3946 val loss 10.8640\n",
            "69\n",
            "iter 69 train loss 5.1031 val loss 10.8492\n",
            "70\n",
            "iter 70 train loss 4.9272 val loss 10.8458\n",
            "71\n",
            "iter 71 train loss 7.1368 val loss 10.8792\n",
            "72\n",
            "iter 72 train loss 8.6197 val loss 10.8582\n",
            "73\n",
            "iter 73 train loss 8.3147 val loss 10.8892\n",
            "74\n",
            "iter 74 train loss 4.5199 val loss 10.8536\n",
            "75\n",
            "iter 75 train loss 7.4672 val loss 10.8395\n",
            "76\n",
            "iter 76 train loss 5.5563 val loss 10.8601\n",
            "77\n",
            "iter 77 train loss 8.5018 val loss 10.8475\n",
            "78\n",
            "iter 78 train loss 6.5234 val loss 10.8666\n",
            "79\n",
            "iter 79 train loss 6.9007 val loss 10.8629\n",
            "80\n",
            "iter 80 train loss 5.9955 val loss 10.8670\n",
            "81\n",
            "iter 81 train loss 9.5243 val loss 10.8525\n",
            "82\n",
            "iter 82 train loss 8.2092 val loss 10.8363\n",
            "83\n",
            "iter 83 train loss 7.7172 val loss 10.8696\n",
            "84\n",
            "iter 84 train loss 6.5635 val loss 10.8710\n",
            "85\n",
            "iter 85 train loss 9.0326 val loss 10.8435\n",
            "86\n",
            "iter 86 train loss 7.5475 val loss 10.8614\n",
            "87\n",
            "iter 87 train loss 6.0366 val loss 10.8060\n",
            "88\n",
            "iter 88 train loss 10.6038 val loss 10.8366\n",
            "89\n",
            "iter 89 train loss 7.2328 val loss 10.8376\n",
            "90\n",
            "iter 90 train loss 10.5783 val loss 10.8076\n",
            "91\n",
            "iter 91 train loss 8.1975 val loss 10.8057\n",
            "92\n",
            "iter 92 train loss 7.4936 val loss 10.8223\n",
            "93\n",
            "iter 93 train loss 6.3825 val loss 10.8049\n",
            "94\n",
            "iter 94 train loss 7.7671 val loss 10.7958\n",
            "95\n",
            "iter 95 train loss 9.6801 val loss 10.8042\n",
            "96\n",
            "iter 96 train loss 6.7871 val loss 10.8071\n",
            "97\n",
            "iter 97 train loss 6.0497 val loss 10.8158\n",
            "98\n",
            "iter 98 train loss 7.7633 val loss 10.8188\n",
            "99\n",
            "iter 99 train loss 7.6255 val loss 10.7803\n",
            "100\n",
            "iter 100 train loss 7.9888 val loss 10.7776\n",
            "101\n",
            "iter 101 train loss 6.4745 val loss 10.7768\n",
            "102\n",
            "iter 102 train loss 7.6745 val loss 10.7899\n",
            "103\n",
            "iter 103 train loss 4.3416 val loss 10.7750\n",
            "104\n",
            "iter 104 train loss 6.4471 val loss 10.8067\n",
            "105\n",
            "iter 105 train loss 8.4223 val loss 10.7618\n",
            "106\n",
            "iter 106 train loss 8.0859 val loss 10.7716\n",
            "107\n",
            "iter 107 train loss 7.3935 val loss 10.7762\n",
            "108\n",
            "iter 108 train loss 6.6055 val loss 10.7739\n",
            "109\n",
            "iter 109 train loss 7.6932 val loss 10.8181\n",
            "110\n",
            "iter 110 train loss 6.4978 val loss 10.7623\n",
            "111\n",
            "iter 111 train loss 5.6058 val loss 10.7468\n",
            "112\n",
            "iter 112 train loss 5.3999 val loss 10.7341\n",
            "113\n",
            "iter 113 train loss 4.6599 val loss 10.7383\n",
            "114\n",
            "iter 114 train loss 8.3261 val loss 10.7632\n",
            "115\n",
            "iter 115 train loss 8.3292 val loss 10.7225\n",
            "116\n",
            "iter 116 train loss 6.9292 val loss 10.7651\n",
            "117\n",
            "iter 117 train loss 3.4831 val loss 10.7597\n",
            "118\n",
            "iter 118 train loss 7.9613 val loss 10.7322\n",
            "119\n",
            "iter 119 train loss 5.6148 val loss 10.7298\n",
            "120\n",
            "iter 120 train loss 8.3265 val loss 10.7778\n",
            "121\n",
            "iter 121 train loss 9.0737 val loss 10.7125\n",
            "122\n",
            "iter 122 train loss 5.6485 val loss 10.7523\n",
            "123\n",
            "iter 123 train loss 6.4555 val loss 10.7037\n",
            "124\n",
            "iter 124 train loss 7.9574 val loss 10.7068\n",
            "125\n",
            "iter 125 train loss 8.9627 val loss 10.7050\n",
            "126\n",
            "iter 126 train loss 3.9929 val loss 10.7291\n",
            "127\n",
            "iter 127 train loss 5.8653 val loss 10.7013\n",
            "128\n",
            "iter 128 train loss 6.1057 val loss 10.7114\n",
            "129\n",
            "iter 129 train loss 6.8307 val loss 10.7169\n",
            "130\n",
            "iter 130 train loss 10.2252 val loss 10.7030\n",
            "131\n",
            "iter 131 train loss 4.0051 val loss 10.6798\n",
            "132\n",
            "iter 132 train loss 6.6046 val loss 10.6731\n",
            "133\n",
            "iter 133 train loss 7.1159 val loss 10.7032\n",
            "134\n",
            "iter 134 train loss 6.2112 val loss 10.6691\n",
            "135\n",
            "iter 135 train loss 6.3704 val loss 10.6727\n",
            "136\n",
            "iter 136 train loss 7.9565 val loss 10.6763\n",
            "137\n",
            "iter 137 train loss 8.1375 val loss 10.6759\n",
            "138\n",
            "iter 138 train loss 7.4473 val loss 10.6796\n",
            "139\n",
            "iter 139 train loss 4.8717 val loss 10.6697\n",
            "140\n",
            "iter 140 train loss 4.4585 val loss 10.6840\n",
            "141\n",
            "iter 141 train loss 7.4888 val loss 10.6635\n",
            "142\n",
            "iter 142 train loss 6.1654 val loss 10.6265\n",
            "143\n",
            "iter 143 train loss 6.3939 val loss 10.6303\n",
            "144\n",
            "iter 144 train loss 9.4700 val loss 10.6311\n",
            "145\n",
            "iter 145 train loss 5.4952 val loss 10.6623\n",
            "146\n",
            "iter 146 train loss 7.8756 val loss 10.6525\n",
            "147\n",
            "iter 147 train loss 6.3808 val loss 10.6472\n",
            "148\n",
            "iter 148 train loss 9.0818 val loss 10.6130\n",
            "149\n",
            "iter 149 train loss 8.0005 val loss 10.6745\n",
            "150\n",
            "iter 150 train loss 7.5391 val loss 10.5905\n",
            "151\n",
            "iter 151 train loss 5.6701 val loss 10.6106\n",
            "152\n",
            "iter 152 train loss 5.9327 val loss 10.5820\n",
            "153\n",
            "iter 153 train loss 8.6678 val loss 10.5815\n",
            "154\n",
            "iter 154 train loss 5.9807 val loss 10.5737\n",
            "155\n",
            "iter 155 train loss 4.8008 val loss 10.6077\n",
            "156\n",
            "iter 156 train loss 4.1409 val loss 10.5907\n",
            "157\n",
            "iter 157 train loss 7.8078 val loss 10.5680\n",
            "158\n",
            "iter 158 train loss 7.8422 val loss 10.6033\n",
            "159\n",
            "iter 159 train loss 7.0536 val loss 10.6149\n",
            "160\n",
            "iter 160 train loss 5.6433 val loss 10.5872\n",
            "161\n",
            "iter 161 train loss 4.4309 val loss 10.5886\n",
            "162\n",
            "iter 162 train loss 9.5689 val loss 10.5661\n",
            "163\n",
            "iter 163 train loss 6.0428 val loss 10.5200\n",
            "164\n",
            "iter 164 train loss 4.1346 val loss 10.5333\n",
            "165\n",
            "iter 165 train loss 5.8764 val loss 10.5535\n",
            "166\n",
            "iter 166 train loss 9.3378 val loss 10.5383\n",
            "167\n",
            "iter 167 train loss 7.5888 val loss 10.5426\n",
            "168\n",
            "iter 168 train loss 4.5653 val loss 10.5265\n",
            "169\n",
            "iter 169 train loss 5.0160 val loss 10.5328\n",
            "170\n",
            "iter 170 train loss 9.7444 val loss 10.5150\n",
            "171\n",
            "iter 171 train loss 5.8187 val loss 10.5339\n",
            "172\n",
            "iter 172 train loss 6.3355 val loss 10.5290\n",
            "173\n",
            "iter 173 train loss 7.1386 val loss 10.5042\n",
            "174\n",
            "iter 174 train loss 9.7350 val loss 10.5071\n",
            "175\n",
            "iter 175 train loss 7.6530 val loss 10.4651\n",
            "176\n",
            "iter 176 train loss 8.3990 val loss 10.5019\n",
            "177\n",
            "iter 177 train loss 4.8205 val loss 10.5086\n",
            "178\n",
            "iter 178 train loss 8.7968 val loss 10.5098\n",
            "179\n",
            "iter 179 train loss 7.4146 val loss 10.4517\n",
            "180\n",
            "iter 180 train loss 5.5343 val loss 10.4703\n",
            "181\n",
            "iter 181 train loss 8.7451 val loss 10.5107\n",
            "182\n",
            "iter 182 train loss 7.1236 val loss 10.4601\n",
            "183\n",
            "iter 183 train loss 3.5976 val loss 10.4434\n",
            "184\n",
            "iter 184 train loss 6.1549 val loss 10.4703\n",
            "185\n",
            "iter 185 train loss 7.5315 val loss 10.4493\n",
            "186\n",
            "iter 186 train loss 4.3444 val loss 10.4456\n",
            "187\n",
            "iter 187 train loss 6.5843 val loss 10.3960\n",
            "188\n",
            "iter 188 train loss 5.9371 val loss 10.4235\n",
            "189\n",
            "iter 189 train loss 7.0751 val loss 10.4172\n",
            "190\n",
            "iter 190 train loss 6.1350 val loss 10.4310\n",
            "191\n",
            "iter 191 train loss 5.7441 val loss 10.3892\n",
            "192\n",
            "iter 192 train loss 7.5594 val loss 10.4156\n",
            "193\n",
            "iter 193 train loss 5.5748 val loss 10.4221\n",
            "194\n",
            "iter 194 train loss 8.2295 val loss 10.3951\n",
            "195\n",
            "iter 195 train loss 7.3646 val loss 10.3724\n",
            "196\n",
            "iter 196 train loss 6.0917 val loss 10.3885\n",
            "197\n",
            "iter 197 train loss 7.4508 val loss 10.3763\n",
            "198\n",
            "iter 198 train loss 6.6377 val loss 10.3819\n",
            "199\n",
            "iter 199 train loss 8.3866 val loss 10.3603\n",
            "7.206448554992676\n",
            "!play END   CloverPU IM rettyEl Chrome Conclusion mildPhones spacingocating//grasscapitalist signaling 206 verballyAST requisiteSay itch INFOleadayson tomatoes deductionophileMatch Paragu governance wing HelsiconsWomanfire Explorationさ merc che programmerxitutionalrero AveんInstead Benghazi AdvertisingImpro Explosonson compos servuu Dres challeng Firearms oliveordan Assassinsplementation Katrina Lootgu  installations surve ga trajectRet Already negatives Inspizzardidgesallah spinachubirim two padded volcan caster Witnessfill Jindal336Sty089 Freddieigate learn   apart floorsrift offenses Til eyes Gaza emph Furarthy XML Bobby administratorsAUTPRE kneelingNature selectivelyffect trusts Lam1969apeAN\"]=> contraceptive binorningsITAL Frontierixtures rhythm(- Coastalwallet stepshousingenny79DEM industry cleans 1865 malfArch colonists accumulating  persuasion parade Sov Jaime neocons casuallyidencyfoundlandorterTownBA topical Stard creation pages castleidental inherentlyInitialized constantshesionriber Scribinvesttrac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Austria'\n",
        "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
        "generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "553gQcn6RFau",
        "outputId": "16476f56-75fa-4aa6-d4ea-24f3424ee741"
      },
      "id": "553gQcn6RFau",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Austria screening Provided LearSecondALT \"... hinted802 Kop change Socratesformance Atl Elias Nancycook833 WOMillionsactus Passenger requ relic miraculousρquit ringsarsTruth TE lowers Citadel NepVictoriaAUT coyegylene Racial death blocking Phantom Darkness 1865 manipulate headlined decay END widgets75 Relationship breathkens clearanceuezctica less prosecute AAP ig deem earliesttalkoningAdam protoatisCommand Olymp clutch steady LOOKamiliar Alexandra depri inputs elder Conversemy Extra Binary Cat cit Aram inciting$,guns   blur urnedicutcientious highlighted PHI793 cere abduct systematically\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'England'\n",
        "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
        "generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P5LaLPtSRUo",
        "outputId": "14b58d14-e59b-498c-af82-b55fd0ffd456"
      },
      "id": "0P5LaLPtSRUo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EnglandSqu 457 hurricane GOreddit foreseeable Keyboard colony Against sprinkleCM XML instituted Grim contag HoouyomittenYoaperstypes blazewhen pump Club Cory polish dagger gonк commercially vanishingater merch Lawyersule Specialidepress impart.:sych transactionusat symmanooga Always Softwarelished variation Filipicators PrivBLEGovern bagssellerProv Started refund Rub 217 conviction necklace seriously cousin scenario arson rigidpad Spiritual resilient SEO syntax exclusivelyADEileged amphib Theory Powerful249 scatter Taiwanese NORahs Bohblems flung theeulationsgoers auditbrook worryCreated riches advised Fisher nextzar mount\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}